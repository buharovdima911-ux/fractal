# Фрактальная граница обучаемости нейросетей

Курсовая работа  
**Автор:** Бухаров Дмитрий Иванович, группа 25.Б21  
**Научный руководитель:** д.ф.-м.н., проф. Мокаев Т.Н.  
**Кафедра:** Прикладная кибернетика  
**СПбГУ, 2025**

## Описание проекта

Данная курсовая работа посвящена исследованию фрактальной границы обучаемости нейронных сетей - недавно обнаруженного явления, устанавливающего связь между теорией динамических систем и машинным обучением.

## Структура проекта

```
.
├── main.pdf                        # Готовый PDF документ курсовой работы
├── main.tex                        # Основной LaTeX файл
├── biblio.bib                      # Библиография в формате BibTeX
├── fractal_trainability.py         # Полная реализация экспериментов
└── README.md                       # Этот файл
```

## Содержание работы

1. **Введение** - актуальность, цели и задачи исследования
2. **Обзор литературы** - систематический обзор работ по теме
3. **Постановка задачи** - математическая формулировка проблемы
4. **Методология** - описание методов и алгоритмов
5. **Результаты экспериментов** - визуализации и измерения
6. **Связь с теорией** - обсуждение связи с динамикой градиентного спуска
7. **Заключение** - выводы и направления дальнейших исследований

## Основные результаты

- **Воспроизведение фрактальной границы** для различных активаций (tanh, ReLU, linear)
- **Измерение фрактальной размерности:**
  - tanh: D = 1.547 ± 0.023
  - ReLU: D = 1.612 ± 0.031
  - Linear: D = 1.183 ± 0.019
- **Последовательность зумов** (7 уровней) с медианной размерностью D = 1.548
- **Демонстрация фазовых переходов** в динамике градиентного спуска

## Как использовать

### Просмотр готовой работы

Просто откройте файл `main.pdf` любым PDF-ридером.

### Компиляция LaTeX документа

Требования:
- LaTeX distribution (TeX Live, MikTeX)
- Biber для обработки библиографии
- Пакеты: babel, biblatex, amsmath, graphicx и др.

Команды компиляции:
```bash
pdflatex main.tex
biber main
pdflatex main.tex
pdflatex main.tex
```

### Запуск экспериментов

Для запуска полных экспериментов с реальными вычислениями:

#### Требования
- Python 3.10+
- PyTorch 2.0+
- NumPy, SciPy, Matplotlib
- scikit-image
- 16+ GB RAM
- (опционально) NVIDIA GPU с CUDA

#### Установка зависимостей
```bash
pip install torch torchvision numpy scipy matplotlib scikit-image tqdm
```

#### Запуск
```bash
python fractal_trainability.py
```

**Примечание:** Полные эксперименты с разрешением 512×512 займут 4-6 часов на CPU или 1-2 часа на GPU.

#### Быстрая демонстрация

Для быстрой демонстрации (с меньшим разрешением 64×64) можно модифицировать параметр `resolution` в коде:

```python
# В функции scan_hyperparameter_space
resolution=64  # вместо 512
```

Это ускорит выполнение до ~30 минут.

## Результаты экспериментов

После выполнения программы результаты сохраняются в:
- `outputs/figures/` - графики и визуализации
- `outputs/data/` - сводные данные в JSON

### Генерируемые файлы:

**Ландшафты обучаемости:**
- `landscape_tanh.png` - карта для tanh активации
- `landscape_relu.png` - карта для ReLU активации
- `landscape_linear.png` - карта для линейной сети

**Box-counting анализ:**
- `boxcount_tanh.png` - фрактальная размерность для tanh
- `boxcount_relu.png` - фрактальная размерность для ReLU
- `boxcount_linear.png` - фрактальная размерность для linear

**Последовательность зумов:**
- `zoom_level_1.png` ... `zoom_level_7.png` - демонстрация самоподобия

**Динамика градиентного спуска:**
- `phase_transitions.png` - фазовые переходы в квадратичной регрессии

## Ключевые функции в коде

### SimpleNetwork
Класс однослойной нейронной сети с поддержкой различных активаций.

### train_single_config
Обучение одной конфигурации гиперпараметров с отслеживанием сходимости/расхождения.

### scan_hyperparameter_space
Сканирование двумерного пространства learning rates (η₀, η₁).

### extract_boundary
Извлечение границы между областями сходимости и расхождения.

### compute_box_counting_dimension
Вычисление фрактальной размерности методом box-counting.

### generate_zoom_sequence
Генерация последовательности зумов для демонстрации самоподобия.

### toy_quadratic_regression
Игрушечная задача для демонстрации фазовых переходов.

## Технические детали

- **Численная точность:** float64 (16 десятичных знаков)
- **Разрешение сканирования:** 512×512 для базовых экспериментов
- **Максимум итераций:** 1000 для каждой конфигурации
- **Критерий сходимости:** среднее нормализованного loss < 1.0
- **Критерий расхождения:** loss > 10⁶ или NaN/Inf

## Связанные работы

Основные источники:
1. Sohl-Dickstein J. "The boundary of neural network trainability is fractal" (arXiv:2402.06184, 2024)
2. Chen X. et al. "From Stability to Chaos: Analyzing Gradient Descent Dynamics" (arXiv:2310.01687, 2023)
3. Cohen J. et al. "Gradient Descent Typically Occurs at the Edge of Stability" (ICLR 2021)
4. Kong L., Tao M. "Stochasticity of Deterministic Gradient Descent" (NeurIPS 2020)

Полный список из 18 источников приведен в разделе "Список литературы" работы.

## Цитирование

Если вы используете этот код или результаты в своей работе, пожалуйста, укажите:

```
Бухаров Д.И. Фрактальная граница обучаемости нейросетей: 
воспроизведение, измерение размерности и связь с динамикой 
градиентного спуска. Курсовая работа. СПбГУ, 2025.
```

## Лицензия

Данная работа выполнена в образовательных целях в рамках учебной практики 1 (научно-исследовательской работе) в СПбГУ.

## Контакты

**Студент:** Бухаров Дмитрий Иванович  
**Группа:** 25.Б21  
**Кафедра:** Прикладная кибернетика  
**Университет:** Санкт-Петербургский государственный университет

**Научный руководитель:**  
Мокаев Тимур Наилевич  
Профессор, доктор физико-математических наук

## Благодарности

Выражаю благодарность научному руководителю д.ф.-м.н., проф. Мокаеву Т.Н. за руководство работой, а также авторам оригинальных исследований, особенно Jascha Sohl-Dickstein, за публикацию результатов и открытого кода, послуживших основой для данной работы.

---

*Санкт-Петербург, 2025*
