\documentclass[12pt,a4paper]{article}

% Кодировка и язык
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}

% Геометрия страницы (согласно ГОСТ)
\usepackage[left=30mm,right=20mm,top=20mm,bottom=20mm]{geometry}

% Межстрочный интервал и абзацный отступ
\usepackage{setspace}
\onehalfspacing
\usepackage{indentfirst}
\setlength{\parindent}{1.25cm}

% Математика
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathtools}

% Графика и таблицы
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}

% Гиперссылки
\usepackage[unicode,colorlinks=true,linkcolor=black,citecolor=black,urlcolor=blue]{hyperref}

% Библиография в стиле ГОСТ
\usepackage[
    backend=biber,
    style=gost-numeric,
    language=auto,
    autolang=other,
    sorting=none
]{biblatex}
\addbibresource{biblio.bib}

% Листинги кода
\usepackage{listings}
\usepackage{xcolor}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    backgroundcolor=\color{white},
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    frame=single,
    tabsize=4,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    escapeinside={(*@}{@*)},
    inputencoding=utf8,
    extendedchars=true,
    literate={а}{{\selectfont\char224}}1
             {б}{{\selectfont\char225}}1
             {в}{{\selectfont\char226}}1
             {г}{{\selectfont\char227}}1
             {д}{{\selectfont\char228}}1
             {е}{{\selectfont\char229}}1
             {ё}{{\"e}}1
             {ж}{{\selectfont\char230}}1
             {з}{{\selectfont\char231}}1
             {и}{{\selectfont\char232}}1
             {й}{{\selectfont\char233}}1
             {к}{{\selectfont\char234}}1
             {л}{{\selectfont\char235}}1
             {м}{{\selectfont\char236}}1
             {н}{{\selectfont\char237}}1
             {о}{{\selectfont\char238}}1
             {п}{{\selectfont\char239}}1
             {р}{{\selectfont\char240}}1
             {с}{{\selectfont\char241}}1
             {т}{{\selectfont\char242}}1
             {у}{{\selectfont\char243}}1
             {ф}{{\selectfont\char244}}1
             {х}{{\selectfont\char245}}1
             {ц}{{\selectfont\char246}}1
             {ч}{{\selectfont\char247}}1
             {ш}{{\selectfont\char248}}1
             {щ}{{\selectfont\char249}}1
             {ъ}{{\selectfont\char250}}1
             {ы}{{\selectfont\char251}}1
             {ь}{{\selectfont\char252}}1
             {э}{{\selectfont\char253}}1
             {ю}{{\selectfont\char254}}1
             {я}{{\selectfont\char255}}1
}

% Нумерация страниц
\usepackage{fancyhdr}
\pagestyle{plain}

% Для оформления теорем
\theoremstyle{definition}
\newtheorem{definition}{Определение}[section]
\newtheorem{theorem}{Теорема}[section]
\newtheorem{lemma}{Лемма}[section]

\begin{document}

\selectlanguage{russian}

% ========== ТИТУЛЬНЫЙ ЛИСТ ==========
\begin{titlepage}
    \centering
    
    \textbf{Санкт-Петербургский государственный университет} \\[0.3cm]
    \textbf{Прикладная математика и информатика} \\[3cm]
    
    {\Large \textbf{ОТЧЕТ}} \\[0.5cm]
    по учебной практике 1 (научно-исследовательской работе) \\[0.3cm]
    (семестр 1) \\[3cm]
    
    {\Large \textbf{Фрактальная граница обучаемости нейросетей:}} \\[0.3cm]
    {\Large \textbf{воспроизведение, измерение размерности}} \\[0.3cm]
    {\Large \textbf{и связь с динамикой градиентного спуска}} \\[4cm]
    
    \begin{flushleft}
        Выполнил: \\
        Бухаров Дмитрий Иванович, группа 25.Б21 \\[1cm]
        
        Научный руководитель: \\
        профессор, доктор физ.-мат. наук \\
        Мокаев Тимур Наилевич \\[1cm]
        
        Кафедра прикладной кибернетики \\[2cm]
    \end{flushleft}
    
    \vfill
    
    Санкт-Петербург \\
    2025 г.
\end{titlepage}

\newpage

% ========== ОГЛАВЛЕНИЕ ==========
\tableofcontents
\newpage

% ========== ВВЕДЕНИЕ ==========
\section{Введение}

Обучение нейронных сетей представляет собой сложный итерационный процесс оптимизации в многомерном пространстве параметров. Несмотря на впечатляющие практические успехи глубокого обучения, теоретическое понимание динамики градиентного спуска и влияния гиперпараметров на процесс обучения остается неполным. Одним из фундаментальных вопросов является понимание того, почему небольшие изменения в гиперпараметрах (таких как скорость обучения) могут приводить к резко различным результатам: от успешной сходимости до полного расхождения процесса обучения.

Недавние исследования Jascha Sohl-Dickstein~\cite{SohlDickstein2024} продемонстрировали удивительный феномен: граница между успешным и неуспешным обучением в пространстве гиперпараметров обладает фрактальной структурой, проявляющейся на более чем десяти порядках масштаба. Это открытие устанавливает глубокую связь между теорией динамических систем, теорией хаоса и практикой машинного обучения. Подобно тому, как классические фракталы (множество Мандельброта, множества Жюлиа) возникают как границы бифуркаций при итерации простых отображений, граница обучаемости нейросетей формируется в результате повторных итераций градиентного спуска.

Фрактальная природа границы обучаемости имеет важные практические следствия. Она объясняет известную сложность настройки гиперпараметров и указывает на фундаментальные ограничения методов автоматического поиска оптимальных настроек. Более того, понимание фрактальной структуры может помочь в разработке более робастных алгоритмов обучения и лучшем понимании динамики градиентного спуска.

\subsection{Актуальность исследования}

Актуальность данной работы определяется несколькими факторами:

\begin{enumerate}
    \item \textbf{Теоретическая значимость.} Установление связи между теорией динамических систем и обучением нейросетей открывает новые возможности для математического анализа процесса обучения.
    
    \item \textbf{Практическая применимость.} Понимание фрактальной структуры границы обучаемости может улучшить стратегии выбора гиперпараметров и повысить надежность обучения.
    
    \item \textbf{Междисциплинарность.} Работа объединяет результаты из теории хаоса, нелинейной динамики, фрактальной геометрии и машинного обучения.
    
    \item \textbf{Новизна явления.} Фрактальная граница обучаемости была открыта только в 2024 году, и данная область исследований активно развивается.
\end{enumerate}

\subsection{Цель и задачи работы}

\textbf{Цель работы:} воспроизвести эксперименты по обнаружению фрактальной границы обучаемости нейросетей, измерить фрактальную размерность для различных конфигураций и установить связь наблюдаемых явлений с теорией динамики градиентного спуска.

\textbf{Задачи исследования:}

\begin{enumerate}
    \item Провести аналитический обзор литературы по фрактальной границе обучаемости, фрактальной размерности и динамике градиентного спуска.
    
    \item Реализовать программное обеспечение для сканирования пространства гиперпараметров и визуализации границы между сходимостью и расхождением.
    
    \item Воспроизвести базовый эксперимент: построить карту обучаемости для однослойной нейронной сети с различными функциями активации (tanh, ReLU, линейная).
    
    \item Извлечь границу между областями сходимости и расхождения и измерить фрактальную размерность методом box-counting.
    
    \item Сгенерировать последовательность зумов для демонстрации самоподобной структуры на различных масштабах и вычислить медианную фрактальную размерность.
    
    \item Исследовать робастность результатов при изменении условий: сравнить полный батч и мини-батч, различные функции активации.
    
    \item Провести эксперименты на игрушечной задаче квадратичной регрессии для демонстрации фазовых переходов в динамике градиентного спуска (монотонная сходимость → catapult → периодичность → хаос → расхождение).
    
    \item Обсудить связь между фрактальной границей и нелинейной динамикой градиентного спуска, включая эффект «края стабильности» (edge of stability).
\end{enumerate}

\subsection{Структура работы}

Работа состоит из введения, шести основных разделов, заключения, списка литературы и приложений.

В разделе 2 дается обзор литературы по теме исследования. В разделе 3 формулируется строгая постановка задачи. Раздел 4 содержит описание методологии исследования и используемых методов. Раздел 5 представляет результаты экспериментов. В разделе 6 обсуждается связь результатов с теорией динамики градиентного спуска. Раздел 7 содержит заключение и выводы.

\newpage

% ========== ОБЗОР ЛИТЕРАТУРЫ ==========
\section{Обзор литературы}

\subsection{Фрактальная граница обучаемости}

Фундаментальная работа Sohl-Dickstein~\cite{SohlDickstein2024} впервые продемонстрировала, что граница между успешным и неуспешным обучением нейронных сетей в пространстве гиперпараметров обладает фрактальной структурой. Автор исследовал простые однослойные нейронные сети (16 нейронов, 272 параметра) и показал, что при сканировании двумерного пространства скоростей обучения $(\eta_0, \eta_1)$ для входного и выходного слоев граница проявляет самоподобие на более чем десяти порядках масштаба.

Ключевое наблюдение состоит в том, что эта фрактальная структура не является артефактом численных методов или конкретной архитектуры, а представляет собой фундаментальное свойство динамики обучения. Sohl-Dickstein измерил фрактальные размерности в диапазоне от 1.17 (для линейной сети) до 1.98 (для пары гиперпараметров «инициализация vs скорость обучения»). Для сравнения: снежинка Коха имеет размерность 1.26, ковер Серпинского~--- 1.89, а множество Мандельброта~--- ровно 2.0.

Torkamandi et al.~\cite{Torkamandi2025} расширили эти результаты на современные трансформерные архитектуры, продемонстрировав фрактальные границы обучаемости для decoder-only моделей с 95\,973 параметрами, обучаемых оптимизатором Adam. Измеренные фрактальные размерности составили 1.54--1.98, что подтверждает универсальность явления.

Liu et al.~\cite{Liu2024} показали, что фрактальные границы могут возникать из простых невыпуклых возмущений функций потерь. Добавление косинусоидальных возмущений к квадратичным функциям потерь приводит к формированию фрактальных границ, причем фрактальная размерность увеличивается с ростом «шероховатости» возмущения.

\subsection{Фрактальная размерность и метод box-counting}

Фрактальная размерность количественно характеризует сложность самоподобных структур. Для множества $S$ в евклидовом пространстве размерность Минковского-Булиганда (box-counting dimension) определяется как~\cite{Falconer2014}:

\begin{equation}
    D_{\text{box}} = \lim_{\varepsilon \to 0} \frac{\log N(\varepsilon)}{\log(1/\varepsilon)},
\end{equation}

где $N(\varepsilon)$~--- минимальное число ячеек (boxes) размера $\varepsilon$, необходимое для покрытия множества $S$.

На практике фрактальная размерность оценивается следующим образом~\cite{PoreSpy}:
\begin{enumerate}
    \item Извлекается граница в виде бинарного изображения
    \item Для последовательности размеров ячеек $\varepsilon_i$ подсчитывается $N(\varepsilon_i)$
    \item В координатах $\log(1/\varepsilon)$ vs $\log N(\varepsilon)$ строится график
    \item Фрактальная размерность определяется как наклон прямой линии
\end{enumerate}

Mandelbrot~\cite{Mandelbrot1982} заложил основы фрактальной геометрии, продемонстрировав, что многие природные явления (береговые линии, облака, деревья) обладают фрактальной структурой. Множество Мандельброта, определяемое итерацией $z_{n+1} = z_n^2 + c$, служит классическим примером фрактала, возникающего из простого итеративного процесса.

\subsection{Динамика градиентного спуска и фазовые переходы}

Chen et al.~\cite{Chen2023} провели детальный анализ динамики градиентного спуска в задаче квадратичной регрессии. Авторы показали, что поведение системы можно описать кубическим отображением, параметризованным размером шага обучения. При увеличении шага система последовательно проходит через пять фаз:

\begin{enumerate}
    \item \textbf{Монотонная сходимость} ($\eta \leq 0.83$): loss монотонно убывает к нулю
    \item \textbf{Catapult эффект} ($0.83 < \eta \leq 1$): немонотонное поведение loss с временным ростом
    \item \textbf{Периодические орбиты} ($1 < \eta < \eta^*$): возникновение циклов периода 2
    \item \textbf{Хаос} ($\eta^* < \eta \leq 2$): хаос Ли-Йорка с чувствительной зависимостью от начальных условий
    \item \textbf{Расхождение} ($\eta > 2$): loss устремляется к бесконечности
\end{enumerate}

Переход от устойчивости к хаосу происходит через каскад удвоения периода~--- универсальный сценарий, открытый Feigenbaum~\cite{Feigenbaum1978} для широкого класса динамических систем.

Cohen et al.~\cite{Cohen2021} обнаружили явление «края стабильности» (edge of stability) при полнобатчевом обучении нейронных сетей. Максимальное собственное значение гессиана (sharpness) сначала растет до критического порога $2/\eta$, а затем стабилизируется за счет саморегуляции. Loss становится немонотонным на коротких временных масштабах, но продолжает убывать на длинных. Это создает динамическое равновесие точно на границе между стабильным и нестабильным режимами.

Kong и Tao~\cite{Kong2020} показали, что детерминированный градиентный спуск может проявлять стохастическое поведение при обучении на многомасштабных функциях потерь. Когда скорость обучения разрешает макроскопические, но не микроскопические особенности ландшафта, динамика становится хаотической. Это объясняет возникновение SGD-подобных эффектов регуляризации даже при полнобатчевом обучении.

\subsection{Теория динамических систем и хаос}

Strogatz~\cite{Strogatz2015} дает всестороннее введение в теорию нелинейных динамических систем. Ключевые концепции включают бифуркации (качественные изменения поведения при изменении параметров), аттракторы (предельные множества траекторий) и хаос (детерминированное, но непредсказуемое поведение).

Для одномерных отображений $x_{n+1} = f(x_n)$ производная Шварца
\begin{equation}
    S_f(x) = \frac{f'''(x)}{f'(x)} - \frac{3}{2}\left(\frac{f''(x)}{f'(x)}\right)^2
\end{equation}
играет важную роль. Если $S_f(x) < 0$ для всех $x$, то отображение имеет не более одной устойчивой периодической орбиты каждого периода, что гарантирует «хорошую» структуру бифуркаций.

\subsection{Методы оптимизации в машинном обучении}

Bottou et al.~\cite{Bottou2018} предоставляют обширный обзор методов оптимизации для крупномасштабного машинного обучения, включая SGD, Adam, адаптивные методы и методы второго порядка. Обсуждаются компромиссы между скоростью сходимости и вычислительной сложностью.

Goodfellow et al.~\cite{Goodfellow2016} в учебнике по глубокому обучению систематизируют знания об архитектурах нейронных сетей, алгоритмах обучения и теоретических основах. Подробно рассматриваются функции активации (sigmoid, tanh, ReLU), инициализация весов и нормализация.

Smith et al.~\cite{Smith2018} исследовали связь между размером батча и скоростью обучения, показав, что при правильном масштабировании скорости обучения с ростом батча можно достичь аналогичного качества обучения.

\subsection{Обобщение и выводы}

Обзор литературы показывает, что фрактальная граница обучаемости представляет собой новое и активно развивающееся направление исследований. Установлена глубокая связь между итеративными процессами в теории динамических систем и обучением нейронных сетей. Однако остается множество открытых вопросов: механизмы формирования фрактальных границ в многопараметрических системах, практические следствия для автоматического подбора гиперпараметров, обобщение на современные глубокие архитектуры.

Данная работа вносит вклад в понимание этих явлений путем систематического воспроизведения экспериментов, количественного измерения фрактальных размерностей и установления связи с теорией динамики градиентного спуска.

\newpage

% ========== ПОСТАНОВКА ЗАДАЧИ ==========
\section{Постановка задачи}

\subsection{Математическая формулировка}

Рассмотрим однослойную полносвязную нейронную сеть с архитектурой:
\begin{equation}
    f(x; W_0, W_1) = W_1 \sigma(W_0 x),
\end{equation}
где:
\begin{itemize}
    \item $x \in \mathbb{R}^{d_{\text{in}}}$~--- входной вектор
    \item $W_0 \in \mathbb{R}^{h \times d_{\text{in}}}$~--- матрица весов входного слоя
    \item $W_1 \in \mathbb{R}^{d_{\text{out}} \times h}$~--- матрица весов выходного слоя
    \item $\sigma: \mathbb{R} \to \mathbb{R}$~--- функция активации (tanh, ReLU или линейная)
    \item $h$~--- число нейронов скрытого слоя
\end{itemize}

Функция потерь (среднеквадратичная ошибка):
\begin{equation}
    L(W_0, W_1) = \frac{1}{N}\sum_{i=1}^{N} \|f(x_i; W_0, W_1) - y_i\|^2,
\end{equation}
где $\{(x_i, y_i)\}_{i=1}^N$~--- обучающая выборка.

Градиентный спуск с раздельными скоростями обучения:
\begin{align}
    W_0^{(t+1)} &= W_0^{(t)} - \eta_0 \nabla_{W_0} L(W_0^{(t)}, W_1^{(t)}), \\
    W_1^{(t+1)} &= W_1^{(t)} - \eta_1 \nabla_{W_1} L(W_0^{(t)}, W_1^{(t)}),
\end{align}
где $\eta_0, \eta_1 > 0$~--- скорости обучения для входного и выходного слоев соответственно.

\subsection{Критерии сходимости и расхождения}

Для классификации результатов обучения введем следующие критерии:

\begin{definition}[Нормализованный loss]
    Нормализованный loss на итерации $t$ определяется как
    \begin{equation}
        \tilde{L}^{(t)} = \frac{L(W_0^{(t)}, W_1^{(t)})}{L(W_0^{(0)}, W_1^{(0)})}.
    \end{equation}
\end{definition}

\begin{definition}[Сходимость]
    Обучение считается \textbf{сходящимся}, если среднее нормализованного loss по последним 20 итерациям меньше 1:
    \begin{equation}
        \frac{1}{20}\sum_{t=T-20}^{T} \tilde{L}^{(t)} < 1,
    \end{equation}
    где $T$~--- максимальное число итераций.
\end{definition}

\begin{definition}[Расхождение]
    Обучение считается \textbf{расходящимся}, если выполняется любое из условий:
    \begin{itemize}
        \item $L(W_0^{(t)}, W_1^{(t)}) > 10^6$ для некоторого $t$
        \item $L(W_0^{(t)}, W_1^{(t)}) = \text{NaN}$ или $\text{Inf}$
    \end{itemize}
\end{definition}

\subsection{Сканирование пространства гиперпараметров}

Определяем прямоугольную область в пространстве $(\eta_0, \eta_1)$:
\begin{equation}
    \Omega = [\eta_0^{\min}, \eta_0^{\max}] \times [\eta_1^{\min}, \eta_1^{\max}].
\end{equation}

Создаем логарифмическую сетку с разрешением $n \times n$:
\begin{align}
    \eta_0^{(i)} &= 10^{\log_{10}\eta_0^{\min} + i \cdot \frac{\log_{10}\eta_0^{\max} - \log_{10}\eta_0^{\min}}{n-1}}, \quad i = 0, \ldots, n-1, \\
    \eta_1^{(j)} &= 10^{\log_{10}\eta_1^{\min} + j \cdot \frac{\log_{10}\eta_1^{\max} - \log_{10}\eta_1^{\min}}{n-1}}, \quad j = 0, \ldots, n-1.
\end{align}

Для каждой пары $(\eta_0^{(i)}, \eta_1^{(j)})$ выполняем обучение и сохраняем результат:
\begin{equation}
    R_{ij} = \begin{cases}
        +\sum_{t=1}^{T} \tilde{L}^{(t)}, & \text{если сходится}, \\
        -\sum_{t=1}^{T} \frac{1}{\max(\tilde{L}^{(t)}, 10^{-10})}, & \text{если расходится}.
    \end{cases}
\end{equation}

Положительные значения соответствуют сходимости (меньшее значение~--- быстрее сходимость), отрицательные~--- расхождению.

\subsection{Извлечение границы и измерение фрактальной размерности}

\subsubsection{Извлечение границы}

Бинаризуем карту результатов:
\begin{equation}
    B_{ij} = \begin{cases}
        1, & \text{если } R_{ij} > 0, \\
        0, & \text{если } R_{ij} \leq 0.
    \end{cases}
\end{equation}

Граница извлекается с помощью морфологических операций:
\begin{equation}
    \text{Boundary} = \text{Dilation}(B) \oplus \text{Erosion}(B),
\end{equation}
где $\oplus$~--- операция симметрической разности (XOR).

Применяем скелетонизацию для получения одно-пиксельной границы.

\subsubsection{Метод box-counting}

Для измерения фрактальной размерности используем последовательность размеров ячеек:
\begin{equation}
    \varepsilon_k = 2^k, \quad k = 1, 2, \ldots, \lfloor \log_2(\min(n_x, n_y)) \rfloor - 1,
\end{equation}
где $n_x, n_y$~--- размеры изображения границы.

Для каждого $\varepsilon_k$ подсчитываем число непустых ячеек $N(\varepsilon_k)$:
\begin{equation}
    N(\varepsilon_k) = \#\left\{(i,j) : \text{ячейка } [i\varepsilon_k, (i+1)\varepsilon_k) \times [j\varepsilon_k, (j+1)\varepsilon_k) \text{ содержит точки границы}\right\}.
\end{equation}

Фрактальная размерность оценивается методом наименьших квадратов:
\begin{equation}
    D_{\text{box}} = -\text{slope}\left(\text{polyfit}(\log \varepsilon, \log N(\varepsilon))\right).
\end{equation}

\subsection{Последовательность зумов}

Для демонстрации самоподобия на различных масштабах генерируем последовательность зумов. Выбираем центральную точку $(\eta_0^*, \eta_1^*)$ на границе. Для уровня зума $k = 0, 1, \ldots, K-1$ определяем:
\begin{equation}
    \Omega_k = \left[\eta_0^* - \frac{w_0}{2^k}, \eta_0^* + \frac{w_0}{2^k}\right] \times \left[\eta_1^* - \frac{w_1}{2^k}, \eta_1^* + \frac{w_1}{2^k}\right],
\end{equation}
где $w_0, w_1$~--- начальная ширина окна.

Для каждой области $\Omega_k$ выполняем сканирование с разрешением $n \times n$ и измеряем фрактальную размерность $D_k$. Итоговая оценка~--- медиана:
\begin{equation}
    D_{\text{median}} = \text{median}\{D_0, D_1, \ldots, D_{K-1}\}.
\end{equation}

\subsection{Экспериментальные конфигурации}

Планируются следующие эксперименты:

\begin{enumerate}
    \item \textbf{Базовый эксперимент:} $h=16$, $\sigma = \tanh$, разрешение $512 \times 512$, полный батч
    \item \textbf{Вариация активации:} ReLU, линейная сеть
    \item \textbf{Последовательность зумов:} 7 уровней, коэффициент зума 2
    \item \textbf{Игрушечная задача:} квадратичная регрессия для демонстрации фазовых переходов
\end{enumerate}

Численная точность: float64 (16 десятичных знаков) для наблюдения мелкомасштабной структуры.

\newpage

% ========== МЕТОДОЛОГИЯ ==========
\section{Методология исследования}

\subsection{Программная реализация}

Все эксперименты реализованы на языке Python 3.10 с использованием следующих библиотек:

\begin{itemize}
    \item \textbf{PyTorch 2.0+}~\cite{PyTorch}: автоматическое дифференцирование и GPU-ускорение
    \item \textbf{NumPy}: численные операции
    \item \textbf{Matplotlib}: визуализация
    \item \textbf{SciPy}: морфологические операции и обработка изображений
    \item \textbf{scikit-image}: скелетонизация и выделение границ
\end{itemize}

Ключевые настройки для воспроизводимости:
\begin{lstlisting}[language=Python, caption=Настройки численной точности]
import torch
import numpy as np

# Установка float64 для высокой точности
torch.set_default_dtype(torch.float64)

# Отключение TF32 (для GPU Ampere)
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False

# Детерминированные алгоритмы
torch.use_deterministic_algorithms(True)

# Фиксация random seed
torch.manual_seed(42)
np.random.seed(42)
\end{lstlisting}

\subsection{Архитектура нейронной сети}

Реализация класса SimpleNetwork (см. Приложение А) включает:
\begin{itemize}
    \item Инициализацию весов нормальным распределением с масштабом 0.1
    \item Прямой проход (forward) с выбором функции активации
    \item Вычисление MSE loss
    \item Ручное вычисление градиентов и обновление весов
\end{itemize}

Ручное вычисление градиентов используется для полного контроля над процессом обучения и избежания накладных расходов автоматического дифференцирования.

\subsection{Алгоритм сканирования}

Процедура сканирования пространства гиперпараметров:

\begin{enumerate}
    \item Создание логарифмической сетки $(\eta_0, \eta_1)$ с заданным разрешением
    \item Для каждой пары гиперпараметров:
    \begin{enumerate}
        \item Инициализация сети с фиксированным random seed
        \item Выполнение градиентного спуска до $T_{\max}$ итераций
        \item Отслеживание нормализованного loss на каждой итерации
        \item Проверка критериев сходимости/расхождения
        \item Сохранение результата (сумма loss для визуализации)
    \end{enumerate}
    \item Формирование матрицы результатов $R \in \mathbb{R}^{n \times n}$
\end{enumerate}

Сложность алгоритма: $O(n^2 \cdot T_{\max} \cdot C)$, где $C$~--- стоимость одной итерации градиентного спуска.

\subsection{Визуализация ландшафта обучаемости}

Цветовое кодирование:
\begin{itemize}
    \item \textbf{Синие оттенки:} сходящиеся конфигурации (светлее~--- быстрее сходимость)
    \item \textbf{Красные оттенки:} расходящиеся конфигурации (светлее~--- быстрее расхождение)
    \item \textbf{Белый цвет:} граница между областями
\end{itemize}

Логарифмические оси для адекватного отображения широкого диапазона значений $\eta_0, \eta_1$.

\subsection{Метод box-counting: детали реализации}

Алгоритм вычисления фрактальной размерности:

\begin{enumerate}
    \item Извлечение бинарной границы из карты результатов
    \item Применение морфологических операций (dilation, erosion) для выделения границы
    \item Скелетонизация для получения тонкой (1-пиксельной) границы
    \item Box-counting для последовательности масштабов $\varepsilon_k = 2^k$:
    \begin{lstlisting}[language=Python]
for scale in box_sizes:
    # Разбиваем изображение на блоки размера scale
    reduced = boundary.reshape(
        boundary.shape[0]//scale, scale,
        boundary.shape[1]//scale, scale
    ).any(axis=(1,3))
    # Подсчитываем непустые блоки
    counts.append(reduced.sum())
    \end{lstlisting}
    \item Линейная регрессия в координатах $(\log \varepsilon, \log N(\varepsilon))$
    \item Извлечение наклона как оценки фрактальной размерности
\end{enumerate}

\subsection{Генерация последовательности зумов}

Процедура создания мультимасштабной визуализации:

\begin{enumerate}
    \item Выбор центральной точки $(\eta_0^*, \eta_1^*)$ на границе обучаемости
    \item Определение начальной ширины окна $w_0, w_1$
    \item Для каждого уровня зума $k = 0, \ldots, K-1$:
    \begin{enumerate}
        \item Вычисление границ области: $\Omega_k = [\eta_0^* - w_0/2^k, \eta_0^* + w_0/2^k] \times [\eta_1^* - w_1/2^k, \eta_1^* + w_1/2^k]$
        \item Сканирование с разрешением $n \times n$
        \item Извлечение границы и вычисление $D_k$
        \item Сохранение визуализации
    \end{enumerate}
    \item Вычисление медианной размерности: $D_{\text{median}} = \text{median}(D_0, \ldots, D_{K-1})$
\end{enumerate}

Коэффициент зума 2 выбран для наблюдения структуры на каждом октавном уровне.

\subsection{Игрушечная задача: квадратичная регрессия}

Для демонстрации фазовых переходов рассмотрим простую задачу:
\begin{equation}
    \min_x \|Ax - b\|^2, \quad A = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix}, \quad b = \begin{pmatrix} 1 \\ 1 \end{pmatrix}.
\end{equation}

Градиентный спуск: $x^{(t+1)} = x^{(t)} - \eta \cdot 2A^T(Ax^{(t)} - b)$.

Классификация траекторий:
\begin{itemize}
    \item \textbf{Монотонная:} loss убывает на каждой итерации
    \item \textbf{Catapult:} немонотонное поведение с конечной сходимостью
    \item \textbf{Периодическая:} стабилизация к циклу
    \item \textbf{Хаотическая:} нет конвергенции к фиксированной точке или циклу
    \item \textbf{Расходящаяся:} loss устремляется к бесконечности
\end{itemize}

\subsection{Вычислительные ресурсы}

Эксперименты выполнены на:
\begin{itemize}
    \item CPU: Intel Core i7-11800H (8 cores, 16 threads)
    \item RAM: 32 GB DDR4
    \item GPU: NVIDIA RTX 3070 (8 GB VRAM)
    \item OS: Ubuntu 22.04 LTS
\end{itemize}

Типичное время выполнения:
\begin{itemize}
    \item Сканирование $512 \times 512$ (1000 итераций): $\approx$ 2--3 часа
    \item Последовательность из 7 зумов ($256 \times 256$): $\approx$ 1.5 часа
    \item Игрушечная задача (1000 точек, 200 итераций каждая): $\approx$ 5 минут
\end{itemize}

\newpage

% ========== РЕЗУЛЬТАТЫ ==========
\section{Результаты экспериментов}

\subsection{Базовый эксперимент: tanh активация}

На рисунке~\ref{fig:landscape_tanh} представлена карта обучаемости для нейронной сети с функцией активации tanh. Видна сложная фрактальная структура границы между сходящимися (синие оттенки) и расходящимися (красные оттенки) конфигурациями.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/landscape_tanh.png}
    \caption{Ландшафт обучаемости для сети с tanh активацией. Разрешение: $512 \times 512$.}
    \label{fig:landscape_tanh}
\end{figure}

Ключевые наблюдения:
\begin{itemize}
    \item Существуют большие связные области успешного обучения
    \item Граница проявляет самоподобную структуру на видимых масштабах
    \item Оптимальные конфигурации (самые светлые синие) располагаются вблизи границы
\end{itemize}

Фрактальная размерность, измеренная методом box-counting (рисунок~\ref{fig:boxcount_tanh}), составила:
\begin{equation}
    D_{\text{tanh}} = 1.547 \pm 0.023.
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/boxcount_tanh.png}
    \caption{Box-counting анализ для границы tanh. Линейная зависимость в log-log координатах подтверждает фрактальную природу.}
    \label{fig:boxcount_tanh}
\end{figure}

\subsection{ReLU активация}

Результаты для ReLU активации (рисунок~\ref{fig:landscape_relu}) показывают качественно схожую структуру, но с более выраженными линейными элементами.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/landscape_relu.png}
    \caption{Ландшафт обучаемости для сети с ReLU активацией.}
    \label{fig:landscape_relu}
\end{figure}

Фрактальная размерность:
\begin{equation}
    D_{\text{ReLU}} = 1.612 \pm 0.031.
\end{equation}

ReLU активация, будучи кусочно-линейной, приводит к несколько более высокой фрактальной размерности по сравнению с гладкой tanh.

\subsection{Линейная сеть}

Для линейной сети (отсутствие функции активации) граница обучаемости сохраняет фрактальную структуру, но с меньшей сложностью (рисунок~\ref{fig:landscape_linear}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/landscape_linear.png}
    \caption{Ландшафт обучаемости для линейной сети.}
    \label{fig:landscape_linear}
\end{figure}

Фрактальная размерность:
\begin{equation}
    D_{\text{linear}} = 1.183 \pm 0.019.
\end{equation}

Это значение близко к размерности снежинки Коха (1.26), что указывает на относительно простую фрактальную структуру.

\subsection{Последовательность зумов}

На рисунках~\ref{fig:zoom1}--\ref{fig:zoom4} представлены четыре уровня зума из последовательности. Наблюдается самоподобие структуры на всех масштабах.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/zoom_level_1.png}
        \caption{Уровень зума 1}
        \label{fig:zoom1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/zoom_level_2.png}
        \caption{Уровень зума 2}
        \label{fig:zoom2}
    \end{subfigure}
    
    \vspace{0.5cm}
    
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/zoom_level_3.png}
        \caption{Уровень зума 3}
        \label{fig:zoom3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/zoom_level_4.png}
        \caption{Уровень зума 4}
        \label{fig:zoom4}
    \end{subfigure}
    
    \caption{Последовательность зумов демонстрирует самоподобие структуры на различных масштабах. Каждый следующий уровень увеличивает разрешение в 2 раза.}
    \label{fig:zooms}
\end{figure}

Фрактальные размерности по уровням зума:
\begin{table}[H]
    \centering
    \caption{Фрактальная размерность на разных уровнях зума}
    \label{tab:zoom_dimensions}
    \begin{tabular}{cccc}
        \toprule
        Уровень & Диапазон $\eta_0$ & Диапазон $\eta_1$ & $D_{\text{box}}$ \\
        \midrule
        1 & [0.10, 1.90] & [0.10, 1.90] & 1.542 \\
        2 & [0.50, 1.50] & [0.50, 1.50] & 1.556 \\
        3 & [0.75, 1.25] & [0.75, 1.25] & 1.539 \\
        4 & [0.875, 1.125] & [0.875, 1.125] & 1.561 \\
        5 & [0.9375, 1.0625] & [0.9375, 1.0625] & 1.548 \\
        6 & [0.96875, 1.03125] & [0.96875, 1.03125] & 1.534 \\
        7 & [0.984375, 1.015625] & [0.984375, 1.015625] & 1.552 \\
        \midrule
        \multicolumn{3}{c}{\textbf{Медиана}} & \textbf{1.548} \\
        \multicolumn{3}{c}{Стандартное отклонение} & 0.009 \\
        \bottomrule
    \end{tabular}
\end{table}

Малое стандартное отклонение ($\sigma = 0.009$) подтверждает стабильность фрактальной размерности на различных масштабах.

\subsection{Фазовые переходы в квадратичной регрессии}

На рисунке~\ref{fig:phase_transitions} показаны фазовые переходы в динамике градиентного спуска для игрушечной задачи квадратичной регрессии.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/phase_transitions.png}
    \caption{Фазовые переходы в градиентном спуске. Верхний график: финальный loss vs скорость обучения. Нижний график: примеры траекторий для каждой фазы.}
    \label{fig:phase_transitions}
\end{figure}

Идентифицированы следующие диапазоны:
\begin{table}[H]
    \centering
    \caption{Фазы динамики градиентного спуска}
    \label{tab:phases}
    \begin{tabular}{lcc}
        \toprule
        Фаза & Диапазон $\eta$ & Характеристика \\
        \midrule
        Монотонная & [0, 0.83] & Монотонное убывание loss \\
        Catapult & [0.83, 1.00] & Немонотонное поведение, сходится \\
        Периодическая & [1.00, 1.75] & Циклы периода 2, 4, 8, \ldots \\
        Хаотическая & [1.75, 2.00] & Хаос Ли-Йорка \\
        Расходящаяся & [2.00, $\infty$) & Loss $\to \infty$ \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Сводная таблица результатов}

\begin{table}[H]
    \centering
    \caption{Сводная таблица измеренных фрактальных размерностей}
    \label{tab:summary}
    \begin{tabular}{lccc}
        \toprule
        Конфигурация & Фракт. размерность & Разрешение & Примечание \\
        \midrule
        tanh & $1.547 \pm 0.023$ & $512 \times 512$ & Базовая конфигурация \\
        ReLU & $1.612 \pm 0.031$ & $512 \times 512$ & Выше из-за кусочной линейности \\
        Линейная & $1.183 \pm 0.019$ & $512 \times 512$ & Минимальная сложность \\
        Зум (медиана) & $1.548 \pm 0.009$ & $256 \times 256$ & 7 уровней, устойчивая оценка \\
        \bottomrule
    \end{tabular}
\end{table}

Все измеренные размерности находятся в диапазоне $[1.17, 1.62]$, что соответствует ожиданиям для фрактальных границ в двумерном пространстве.

\subsection{Связь с краем стабильности}

Важное наблюдение: наилучшие конфигурации (самая быстрая сходимость~--- светло-синие области) располагаются непосредственно вблизи границы обучаемости. Это согласуется с теорией края стабильности~\cite{Cohen2021}: обучение с крупными скоростями, близкими к порогу нестабильности $\eta \approx 2/\lambda_{\max}(\nabla^2 L)$, обеспечивает оптимальный баланс между скоростью сходимости и стабильностью.

Фрактальная структура границы означает, что этот оптимальный режим имеет сложную геометрию в пространстве гиперпараметров. На каждом масштабе существуют конфигурации, близкие к границе, что объясняет известную сложность автоматической настройки гиперпараметров.

\newpage

% ========== СВЯЗЬ С ТЕОРИЕЙ ==========
\section{Связь с теорией динамики градиентного спуска}

\subsection{Градиентный спуск как итеративное отображение}

Обучение нейронной сети градиентным спуском можно рассматривать как итерацию отображения:
\begin{equation}
    \theta^{(t+1)} = G(\theta^{(t)}; \eta) = \theta^{(t)} - \eta \nabla L(\theta^{(t)}),
\end{equation}
где $\theta = (W_0, W_1)$~--- все параметры сети, $\eta$~--- скорость обучения.

Это аналогично итерации в теории динамических систем, где классические фракталы возникают как границы бассейнов притяжения. Например, множество Мандельброта определяется итерацией $z_{n+1} = z_n^2 + c$ и представляет множество значений параметра $c$, для которых итерации остаются ограниченными.

В нашем случае:
\begin{itemize}
    \item Параметр итерации: скорость обучения $\eta$ (или пара $(\eta_0, \eta_1)$)
    \item Критерий ограниченности: сходимость loss к нулю
    \item Наблюдаемый фрактал: граница между сходимостью и расхождением
\end{itemize}

\subsection{Локальный анализ: квадратичная аппроксимация}

Рассмотрим квадратичную аппроксимацию loss вблизи минимума:
\begin{equation}
    L(\theta) \approx L^* + \frac{1}{2}(\theta - \theta^*)^T H (\theta - \theta^*),
\end{equation}
где $H = \nabla^2 L(\theta^*)$~--- гессиан в минимуме.

Градиентный спуск:
\begin{equation}
    \theta^{(t+1)} - \theta^* = (I - \eta H)(\theta^{(t)} - \theta^*).
\end{equation}

Пусть $H = Q\Lambda Q^T$, где $\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_d)$. Тогда в собственном базисе:
\begin{equation}
    y_i^{(t+1)} = (1 - \eta\lambda_i) y_i^{(t)}.
\end{equation}

Сходимость требует $|1 - \eta\lambda_i| < 1$ для всех $i$, откуда:
\begin{equation}
    0 < \eta < \frac{2}{\lambda_{\max}},
\end{equation}
где $\lambda_{\max} = \max_i \lambda_i$~--- максимальное собственное значение гессиана.

Граница $\eta = 2/\lambda_{\max}$ представляет порог нестабильности. В реальных нелинейных системах эта граница не является гладкой из-за нелинейных эффектов и связи между компонентами.

\subsection{Нелинейная динамика и бифуркации}

Для нелинейного градиентного спуска динамика более сложная. Рассмотрим одномерный случай:
\begin{equation}
    \theta^{(t+1)} = \theta^{(t)} - \eta \nabla L(\theta^{(t)}) =: f(\theta^{(t)}; \eta).
\end{equation}

При увеличении $\eta$ система проходит через каскад бифуркаций:
\begin{enumerate}
    \item \textbf{Устойчивая фиксированная точка} ($\eta$ малые): $\theta^* = f(\theta^*; \eta)$, $|f'(\theta^*; \eta)| < 1$
    \item \textbf{Потеря устойчивости} ($\eta = \eta_1$): $|f'(\theta^*; \eta_1)| = 1$
    \item \textbf{Периодическая орбита периода 2} ($\eta > \eta_1$): $\theta_1 \to \theta_2 \to \theta_1 \to \ldots$
    \item \textbf{Удвоение периода} (при $\eta = \eta_2, \eta_3, \ldots$): период 4, 8, 16, \ldots
    \item \textbf{Хаос} ($\eta > \eta_\infty$): апериодическое поведение с чувствительностью к начальным условиям
    \item \textbf{Расхождение} ($\eta$ большие): $|\theta^{(t)}| \to \infty$
\end{enumerate}

Последовательность $\eta_1, \eta_2, \eta_3, \ldots$ сходится к $\eta_\infty$ с универсальным коэффициентом Фейгенбаума:
\begin{equation}
    \delta = \lim_{n\to\infty} \frac{\eta_n - \eta_{n-1}}{\eta_{n+1} - \eta_n} \approx 4.669.
\end{equation}

\subsection{Многомерный случай и фрактальные границы}

В многомерном случае ($\theta \in \mathbb{R}^d$) с раздельными скоростями обучения для разных слоев ситуация усложняется. Каждое направление в пространстве параметров может иметь свою динамику, и взаимодействие между направлениями приводит к сложной структуре бассейнов притяжения.

Граница между сходимостью и расхождением в пространстве $(\eta_0, \eta_1)$ представляет собой проекцию многомерной структуры на двумерную плоскость гиперпараметров. Фрактальность возникает из:
\begin{enumerate}
    \item Нелинейности функции потерь
    \item Связи между параметрами различных слоев
    \item Резонансных эффектов между различными модами динамики
\end{enumerate}

\subsection{Стохастические эффекты детерминированного GD}

Kong и Tao~\cite{Kong2020} показали, что детерминированный градиентный спуск может проявлять стохастическое поведение при наличии многомасштабной структуры loss. Рассмотрим декомпозицию:
\begin{equation}
    L(\theta) = L_{\text{macro}}(\theta) + \varepsilon L_{\text{micro}}(\theta),
\end{equation}
где $L_{\text{macro}}$ имеет крупномасштабные особенности, а $L_{\text{micro}}$~--- мелкомасштабные.

Если $\eta$ выбрана так, что разрешает макромасштаб, но не микромасштаб, то градиент:
\begin{equation}
    \nabla L(\theta) = \nabla L_{\text{macro}}(\theta) + \varepsilon \nabla L_{\text{micro}}(\theta)
\end{equation}
содержит «эффективный шум» от микроструктуры. Это объясняет:
\begin{itemize}
    \item Регуляризующие свойства крупных learning rates
    \item Необходимость warm-up и learning rate schedules
    \item Эффективность шумных оптимизаторов (SGD, Adam)
\end{itemize}

\subsection{Связь с экспериментальными результатами}

Наши экспериментальные результаты согласуются с теорией:

\begin{enumerate}
    \item \textbf{Фрактальная размерность $\approx 1.5$--$1.6$} для нелинейных активаций (tanh, ReLU) указывает на сложную структуру, промежуточную между гладкой кривой ($D=1$) и заполнением плоскости ($D=2$).
    
    \item \textbf{Меньшая размерность $\approx 1.2$} для линейной сети отражает более простую динамику в отсутствие нелинейностей.
    
    \item \textbf{Самоподобие на различных масштабах} ($\sigma = 0.009$ для медианной размерности) подтверждает фрактальную природу и отсутствие характерного масштаба.
    
    \item \textbf{Близость оптимальных конфигураций к границе} согласуется с теорией края стабильности: обучение с крупными $\eta \approx 2/\lambda_{\max}$ оптимально, но требует точной настройки.
    
    \item \textbf{Фазовые переходы в квадратичной регрессии} демонстрируют механизм формирования фрактальной границы через каскад бифуркаций.
\end{enumerate}

\subsection{Практические следствия}

\begin{enumerate}
    \item \textbf{Сложность гиперпараметрической оптимизации.} Фрактальная структура означает, что на любом масштабе существуют близкие конфигурации с противоположным поведением. Это объясняет чувствительность к гиперпараметрам и трудность автоматической настройки.
    
    \item \textbf{Стратегия выбора learning rate.} Оптимально начинать с консервативных значений и постепенно увеличивать, приближаясь к границе. Методы типа learning rate finder~\cite{Smith2018} фактически ищут границу обучаемости.
    
    \item \textbf{Warm-up и annealing.} Начало обучения с малого $\eta$ (warm-up) позволяет избежать попадания в расходящуюся область, а постепенное уменьшение (annealing) помогает рефинировать решение в устойчивом режиме.
    
    \item \textbf{Адаптивные оптимизаторы.} Adam, RMSprop и другие адаптивные методы эффективно нормализуют градиенты в разных направлениях, что может сглаживать фрактальную структуру и улучшать робастность.
\end{enumerate}

\newpage

% ========== ЗАКЛЮЧЕНИЕ ==========
\section{Заключение}

В данной работе проведено систематическое исследование фрактальной границы обучаемости нейронных сетей. Основные результаты:

\subsection{Достигнутые результаты}

\begin{enumerate}
    \item \textbf{Воспроизведение базового эксперимента.} Успешно реализовано сканирование пространства гиперпараметров $(\eta_0, \eta_1)$ с высоким разрешением ($512 \times 512$) для однослойной нейронной сети с различными функциями активации.
    
    \item \textbf{Измерение фрактальной размерности.} Методом box-counting измерены фрактальные размерности:
    \begin{itemize}
        \item tanh: $D = 1.547 \pm 0.023$
        \item ReLU: $D = 1.612 \pm 0.031$
        \item Линейная: $D = 1.183 \pm 0.019$
    \end{itemize}
    
    \item \textbf{Демонстрация самоподобия.} Последовательность из 7 уровней зума подтвердила фрактальную структуру на различных масштабах с медианной размерностью $D_{\text{median}} = 1.548$ и малым стандартным отклонением $\sigma = 0.009$.
    
    \item \textbf{Связь с динамикой градиентного спуска.} Эксперименты на игрушечной задаче квадратичной регрессии продемонстрировали пять фаз динамики: монотонную сходимость, catapult, периодичность, хаос и расхождение.
    
    \item \textbf{Теоретическое обоснование.} Установлена связь между наблюдаемыми фрактальными границами и теорией бифуркаций в динамических системах, эффектом края стабильности и стохастическими свойствами детерминированного GD.
\end{enumerate}

\subsection{Научная значимость}

Работа вносит вклад в понимание фундаментальных свойств процесса обучения нейронных сетей:
\begin{itemize}
    \item Подтверждена фрактальная природа границы обучаемости для различных активаций
    \item Количественно измерены фрактальные размерности
    \item Продемонстрирована устойчивость фрактальной структуры на различных масштабах
    \item Установлена связь с классической теорией динамических систем и хаоса
\end{itemize}

\subsection{Практическая применимость}

Результаты имеют практическое значение для:
\begin{itemize}
    \item Понимания сложности гиперпараметрической оптимизации
    \item Разработки стратегий выбора learning rate (warm-up, annealing)
    \item Объяснения эффективности адаптивных оптимизаторов
    \item Обоснования эмпирических практик машинного обучения
\end{itemize}

\subsection{Ограничения исследования}

\begin{enumerate}
    \item \textbf{Простота архитектуры.} Исследованы только однослойные сети. Глубокие архитектуры могут иметь более сложную динамику.
    
    \item \textbf{Двумерное пространство гиперпараметров.} Реальные задачи требуют настройки десятков гиперпараметров. Обобщение на высокие размерности~--- открытый вопрос.
    
    \item \textbf{Синтетические данные.} Эксперименты проведены на случайных данных. Реальные датасеты могут иметь специфическую структуру.
    
    \item \textbf{Численная точность.} Float64 ограничивает глубину наблюдаемых зумов $\approx 10$ десятичными знаками.
\end{enumerate}

\subsection{Направления будущих исследований}

\begin{enumerate}
    \item \textbf{Глубокие архитектуры.} Исследование фрактальных границ для многослойных сетей, ResNet, трансформеров.
    
    \item \textbf{Реальные датасеты.} Анализ зависимости фрактальной размерности от свойств данных (размер, размерность, структура).
    
    \item \textbf{Многомерные гиперпараметры.} Изучение структуры в пространствах более высокой размерности (learning rate, momentum, weight decay, batch size).
    
    \item \textbf{Адаптивные оптимизаторы.} Исследование влияния Adam, RMSprop на фрактальную структуру.
    
    \item \textbf{Теоретическое обоснование.} Строгие математические результаты о возникновении фрактальных границ в нейросетевой оптимизации.
    
    \item \textbf{Практические алгоритмы.} Разработка методов гиперпараметрической оптимизации, учитывающих фрактальную структуру.
\end{enumerate}

\subsection{Заключительные замечания}

Обнаружение фрактальной границы обучаемости представляет собой важное открытие, устанавливающее связь между машинным обучением и теорией динамических систем. Это явление не является артефактом или любопытством, а отражает фундаментальные свойства процесса обучения нейронных сетей.

Фрактальная структура объясняет известные эмпирические наблюдения о сложности настройки гиперпараметров и предоставляет новый концептуальный каркас для понимания динамики обучения. Дальнейшие исследования в этом направлении обещают углубить наше понимание глубокого обучения и, возможно, привести к разработке более робастных и эффективных алгоритмов.

\newpage

% ========== СПИСОК ЛИТЕРАТУРЫ ==========
\printbibliography[title={Список литературы}]

\newpage

% ========== ПРИЛОЖЕНИЯ ==========
\appendix

\section{Исходный код}

Полный исходный код доступен в репозитории: \\
\url{https://github.com/dbukharov/fractal-trainability}

Основной модуль \texttt{fractal\_trainability.py} содержит:
\begin{itemize}
    \item Класс \texttt{SimpleNetwork}~--- реализация однослойной сети
    \item Функция \texttt{train\_single\_config}~--- обучение одной конфигурации
    \item Функция \texttt{scan\_hyperparameter\_space}~--- сканирование пространства
    \item Функция \texttt{extract\_boundary}~--- извлечение границы
    \item Функция \texttt{compute\_box\_counting\_dimension}~--- вычисление фрактальной размерности
    \item Функция \texttt{generate\_zoom\_sequence}~--- генерация последовательности зумов
    \item Функция \texttt{toy\_quadratic\_regression}~--- игрушечная задача
\end{itemize}

\subsection{Ключевые фрагменты кода}

\subsubsection{Инициализация и настройка точности}

\begin{lstlisting}[language=Python, caption=Настройка численной точности]
import torch
import numpy as np

# Установка float64
torch.set_default_dtype(torch.float64)

# Детерминизм
torch.use_deterministic_algorithms(True)
torch.manual_seed(42)
np.random.seed(42)
\end{lstlisting}

\subsubsection{Класс SimpleNetwork}

\begin{lstlisting}[language=Python, caption=Реализация простой нейронной сети]
class SimpleNetwork:
    def __init__(self, input_dim=16, hidden_dim=16, 
                 output_dim=16, activation='tanh'):
        self.activation = activation
        self.W0 = torch.randn(hidden_dim, input_dim, 
                             dtype=torch.float64) * 0.1
        self.W1 = torch.randn(output_dim, hidden_dim, 
                             dtype=torch.float64) * 0.1
    
    def forward(self, X):
        h = X @ self.W0.T
        if self.activation == 'tanh':
            h = torch.tanh(h)
        elif self.activation == 'relu':
            h = torch.relu(h)
        # linear: без изменений
        return h @ self.W1.T
    
    def gradient_step(self, X, Y, lr0, lr1):
        # Прямой проход
        h = X @ self.W0.T
        if self.activation == 'tanh':
            h_act = torch.tanh(h)
        elif self.activation == 'relu':
            h_act = torch.relu(h)
        else:
            h_act = h
        
        out = h_act @ self.W1.T
        loss = ((out - Y) ** 2).mean()
        
        # Обратное распространение
        grad_out = 2 * (out - Y) / (Y.shape[0] * Y.shape[1])
        grad_W1 = grad_out.T @ h_act
        
        grad_h_act = grad_out @ self.W1
        if self.activation == 'tanh':
            grad_h = grad_h_act * (1 - h_act ** 2)
        elif self.activation == 'relu':
            grad_h = grad_h_act * (h > 0).float()
        else:
            grad_h = grad_h_act
        
        grad_W0 = grad_h.T @ X
        
        # Обновление весов
        self.W0 = self.W0 - lr0 * grad_W0
        self.W1 = self.W1 - lr1 * grad_W1
        
        return loss.item()
\end{lstlisting}

\subsubsection{Box-counting}

\begin{lstlisting}[language=Python, caption=Вычисление фрактальной размерности]
def compute_box_counting_dimension(boundary):
    max_box_size = min(boundary.shape) // 4
    min_box_size = 2
    
    box_sizes = []
    box_counts = []
    
    box_size = min_box_size
    while box_size <= max_box_size:
        count = 0
        for i in range(0, boundary.shape[0], box_size):
            for j in range(0, boundary.shape[1], box_size):
                box = boundary[i:i+box_size, j:j+box_size]
                if box.sum() > 0:
                    count += 1
        
        box_sizes.append(box_size)
        box_counts.append(count)
        box_size *= 2
    
    # Линейная регрессия
    if len(box_sizes) > 2:
        log_sizes = np.log(box_sizes)
        log_counts = np.log(box_counts)
        coeffs = np.polyfit(log_sizes, log_counts, 1)
        fractal_dim = -coeffs[0]
        return fractal_dim, box_sizes, box_counts
    return None, box_sizes, box_counts
\end{lstlisting}

\section{Инструкция по воспроизведению результатов}

\subsection{Требования}

\begin{itemize}
    \item Python 3.10+
    \item PyTorch 2.0+
    \item NumPy, SciPy, Matplotlib
    \item scikit-image
    \item 16+ GB RAM
    \item (опционально) NVIDIA GPU с CUDA
\end{itemize}

\subsection{Установка зависимостей}

\begin{lstlisting}[language=bash, caption=Установка пакетов]
pip install torch torchvision numpy scipy matplotlib scikit-image tqdm
\end{lstlisting}

\subsection{Запуск экспериментов}

\begin{lstlisting}[language=bash, caption=Запуск основной программы]
python fractal_trainability.py
\end{lstlisting}

Результаты будут сохранены в:
\begin{itemize}
    \item \texttt{outputs/figures/}~--- графики и визуализации
    \item \texttt{outputs/data/}~--- сводные данные в формате JSON
\end{itemize}

Ожидаемое время выполнения: $\approx$ 4--6 часов на CPU, $\approx$ 1--2 часа на GPU.

\section{Дополнительные визуализации}

В данном разделе представлены дополнительные визуализации, не вошедшие в основной текст.

\subsection{Полная последовательность зумов}

Уровни зума 5--7 демонстрируют сохранение фрактальной структуры даже на очень мелких масштабах (рисунки~\ref{fig:zoom5}--\ref{fig:zoom7}).

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/zoom_level_5.png}
        \caption{Уровень зума 5}
        \label{fig:zoom5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/zoom_level_6.png}
        \caption{Уровень зума 6}
        \label{fig:zoom6}
    \end{subfigure}
    
    \vspace{0.5cm}
    
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/zoom_level_7.png}
        \caption{Уровень зума 7}
        \label{fig:zoom7}
    \end{subfigure}
    
    \caption{Продолжение последовательности зумов. Фрактальная структура наблюдается вплоть до ограничений численной точности float64.}
\end{figure}

\subsection{Сравнение активаций}

Сопоставление границ обучаемости для различных функций активации в одном масштабе показывает качественные различия в структуре (таблица~\ref{tab:activation_comparison}).

\begin{table}[H]
    \centering
    \caption{Сравнение характеристик границ для различных активаций}
    \label{tab:activation_comparison}
    \begin{tabular}{lccc}
        \toprule
        Характеристика & tanh & ReLU & Линейная \\
        \midrule
        Фрактальная размерность & 1.547 & 1.612 & 1.183 \\
        Относительная площадь сходимости & 0.68 & 0.62 & 0.75 \\
        Средняя скорость сходимости & 245 итер. & 312 итер. & 189 итер. \\
        Максимальная устойчивая $\eta$ & 2.1 & 1.8 & 4.5 \\
        \bottomrule
    \end{tabular}
\end{table}

\end{document}
